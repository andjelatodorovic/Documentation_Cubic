

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Welcome to the VLML documentation! &mdash; Video Library - Machine Learning 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> Video Library - Machine Learning
          

          
            
            <img src="_static/CUBIC.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Welcome to the VLML documentation!</a></li>
<li><a class="reference internal" href="#installation-process">Installation process</a></li>
<li><a class="reference internal" href="#run-detector">Run detector</a></li>
<li><a class="reference internal" href="#modifying-the-code">Modifying the code</a><ul>
<li><a class="reference internal" href="#changing-the-detect-file">Changing the detect file</a></li>
<li><a class="reference internal" href="#changing-the-darknet-file">Changing the darknet file</a></li>
</ul>
</li>
<li><a class="reference internal" href="#training-on-custom-dataset">Training on custom dataset</a><ul>
<li><a class="reference internal" href="#training-from-scratch-on-the-custom-dataset">Training from scratch on the custom dataset</a></li>
<li><a class="reference internal" href="#fine-tuning-pre-trained-model-on-custom-dataset">Fine-tuning pre-trained model on custom dataset</a></li>
<li><a class="reference internal" href="#my-experiment-with-fine-tuning-on-vedai-dataset">My experiment with fine-tuning on VEDAI dataset</a></li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-search">Indices and search</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Video Library - Machine Learning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Welcome to the VLML documentation!</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="welcome-to-the-vlml-documentation">
<h1>Welcome to the VLML documentation!<a class="headerlink" href="#welcome-to-the-vlml-documentation" title="Permalink to this headline">¶</a></h1>
<p>Machine learning submodule in Video Library project handles the problem of Automatic Video Tag Suggestion and Content Categorization.
It uses the network architecture of YOLOv3 (You Only Look Once) object detector (<a class="reference external" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">paper</a>)
fine-tuned on the <a class="reference external" href="https://downloads.greyc.fr/vedai/">VEDAI</a>. dataset, and pre-trained on <a class="reference external" href="https://storage.googleapis.com/openimages/web/index.html">Google Open Images v5 dataset</a>.</p>
</div>
<div class="section" id="installation-process">
<h1>Installation process<a class="headerlink" href="#installation-process" title="Permalink to this headline">¶</a></h1>
<dl class="docutils">
<dt>Requirements if running on local machine:</dt>
<dd><ul class="first last">
<li><p class="first">Python 3.6 or higher and Anaconda installed</p>
</li>
<li><p class="first">CUDA (tested on CUDA 10.1) <a class="reference external" href="https://developer.nvidia.com/cuda-downloads">Download it here</a></p>
</li>
<li><p class="first">CDNN (note that CUDNN must match the CUDA version) <a class="reference external" href="https://developer.nvidia.com/cudnn">Download it here</a></p>
</li>
<li><dl class="first docutils">
<dt>PyTorch 0.4 or higher (Please note that using PyTorch 0.3 will effect the detector)</dt>
<dd><p class="first last">If using Anaconda you can run
<code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">pytorch</span> <span class="pre">torchvision</span> <span class="pre">cudatoolkit=10.1</span> <span class="pre">-c</span> <span class="pre">pytorch</span></code> (change to your CUDA Toolkit version)</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Necessary libraries (one of the following options):</dt>
<dd><ol class="first arabic simple">
<li><dl class="first docutils">
<dt>Custom Anaconda yolo environment (<em>yolo.yml</em>)</dt>
<dd><ul class="first last">
<li>Download the yolo.yml file from <a class="reference external" href="https://drive.google.com/open?id=131TYV34-pQv7SrjvX8-C7syYHIHqWr-d">here</a></li>
<li>Run <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">env</span> <span class="pre">create</span> <span class="pre">-f</span> <span class="pre">yolo.yml</span></code></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Installing the libraries from PyPi (<em>requirements.txt</em>)</dt>
<dd><ul class="first last">
<li>Download the requirements.txt file from <a class="reference external" href="https://drive.google.com/open?id=1JJ2RDYWuYv_pmK8wah2-pNwuBxgOTZ-H">here</a></li>
<li>Run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">requirements.txt</span></code></li>
</ul>
</dd>
</dl>
</li>
</ol>
<p class="last">Please note that some of the libraries may not have been installed through requirements.
If openCV is not installed, please run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">opencv-python</span></code></p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Requirements for running in custom Docker container:</dt>
<dd><ul class="first last simple">
<li>Get CUDA running on Docker (nvidia-smi)</li>
<li>Docker file (DockerFileML)</li>
</ul>
</dd>
</dl>
<p>Weights files can be found here <a class="reference external" href="https://drive.google.com/open?id=1A52PSNSCN2hgsjaT8zGxujgwvLRMpQZ6">YOLO on COCO</a> and here <a class="reference external" href="https://drive.google.com/open?id=1PIYyZcLblizLjbNbXb2oUgDoq-lboo01">YOLO on Custom Dataset</a></p>
</div>
<div class="section" id="run-detector">
<h1>Run detector<a class="headerlink" href="#run-detector" title="Permalink to this headline">¶</a></h1>
<p>The object detector is integrated into the Video Library project, but can run individualy on any video input.</p>
<p>Before running the detector, you can check if CUDA is active by running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUDA_LAUNCH_BLOCKING&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</pre></div>
</div>
<p>When not using the whole video library system, one can run the detection from command line, running: <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">detect_cmd.py</span></code> with optional parameters <code class="docutils literal notranslate"><span class="pre">--videoid</span> <span class="pre">(identifying</span> <span class="pre">hash</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">video)</span></code> <code class="docutils literal notranslate"><span class="pre">--video</span> <span class="pre">(path</span> <span class="pre">to</span> <span class="pre">video</span> <span class="pre">file)</span></code> <code class="docutils literal notranslate"><span class="pre">--batch_size</span> <span class="pre">(batch</span> <span class="pre">size)</span></code> <code class="docutils literal notranslate"><span class="pre">--confidence</span> <span class="pre">(confidence)</span></code> <code class="docutils literal notranslate"><span class="pre">--nms_thresh</span> <span class="pre">(threshold)</span></code></p>
</div>
<div class="section" id="modifying-the-code">
<h1>Modifying the code<a class="headerlink" href="#modifying-the-code" title="Permalink to this headline">¶</a></h1>
<div class="section" id="changing-the-detect-file">
<h2>Changing the detect file<a class="headerlink" href="#changing-the-detect-file" title="Permalink to this headline">¶</a></h2>
<p>The main function is located in detect.py file. It is based on the Darknet Implementation <a class="reference external" href="(https://pjreddie.com/darknet/yolo/)">here</a>
<em>Important note</em>: By default, Darknet changes the resolution of input images to the size of 416 x 416. You can change this in <em>darknet.py</em> by changing this line of code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="p">(</span><span class="mi">416</span><span class="p">,</span><span class="mi">416</span><span class="p">)</span>
</pre></div>
</div>
<p>but simultaneously changing resolution in <em>detect.py</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reso</span> <span class="o">=</span> <span class="mi">416</span>
</pre></div>
</div>
<p>Again,  whatever value you chose, rememeber it should be a <strong>multiple of 32 and greater than 32</strong>.</p>
<p><em>On different scales</em>: YOLO v3 makes detections across different scales, each of which deputise in detecting objects of different sizes depending upon whether they capture coarse features, fine grained features or something between. You can experiment with these scales changing this line</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scales</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span>
</pre></div>
</div>
<p>There are three <strong>important files</strong> that are not to be missed: weights file (<em>yolo-openimages.weights</em>), cfg file (<em>cfg/yolov3-openimages.cfg</em>) and names file (<em>data/openimages.names</em>).</p>
<p>If needed, you can update these files by chaning following lines of code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">weights</span> <span class="o">=</span> <span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span><span class="s1">&#39;yolov3-openimages.weights&#39;</span><span class="p">)</span>
<span class="n">cfgfile</span> <span class="o">=</span> <span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span><span class="s1">&#39;cfg&#39;</span><span class="p">,</span><span class="s1">&#39;yolo3-openimages.cfg&#39;</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">load_classes</span><span class="p">(</span><span class="n">osp</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;openimages.names&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>Please note that any changes of the following files could change the input tensor size and lead to an error, so please change cfg and weights accordingly if training on the custom dataset, and change classes in the names file.</p>
</div>
<div class="section" id="changing-the-darknet-file">
<h2>Changing the darknet file<a class="headerlink" href="#changing-the-darknet-file" title="Permalink to this headline">¶</a></h2>
<p>Darknet file consists of the following parts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_cfg</span><span class="p">(</span><span class="n">cfgfile</span><span class="p">):</span>
</pre></div>
</div>
<p>Gets the cfg file and stores each block as a dict. The attributes of the blocks and their values are stored as key-value pairs in the dictionary.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MaxPoolStride1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
</div>
<p>Controls how the filter convolves around the input volume</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Upsample</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
</div>
<p>Controls how the filter convolves around the input volume</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EmptyLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
</div>
<p><em>Important note:</em> When using torch.cat we put a dummy layer in the place of a proposed route layer, and then perform the concatenation directly in the forward function of the nn.Module object representing darknet. <strong>Please do not remove this layer</strong>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Darknet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</pre></div>
</div>
<p>Performs operation on the layers in the YOLO architecture.</p>
<p>It is highly recommended not to change the Darknet function, since it behaves only as an additional wrapper for implementing behavior of the layers.
Note that if there is no CUDA available, we don’t convert the parameter to GPU.Instead. we copy the parameter and then convert it to CPU</p>
</div>
</div>
<div class="section" id="training-on-custom-dataset">
<h1>Training on custom dataset<a class="headerlink" href="#training-on-custom-dataset" title="Permalink to this headline">¶</a></h1>
<div class="section" id="training-from-scratch-on-the-custom-dataset">
<h2>Training from scratch on the custom dataset<a class="headerlink" href="#training-from-scratch-on-the-custom-dataset" title="Permalink to this headline">¶</a></h2>
<p>1. <em>Important</em>: Please make sure your data is in Darknet format</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">..</span><span class="n">dataset</span>\<span class="n">images</span>\<span class="n">val</span>\<span class="n">image1</span><span class="o">.</span><span class="n">jpg</span>  <span class="c1"># image</span>
<span class="o">..</span><span class="n">dataset</span>\<span class="n">labels</span>\<span class="n">val</span>\<span class="n">image1</span><span class="o">.</span><span class="n">txt</span>  <span class="c1"># label</span>
</pre></div>
</div>
<dl class="docutils">
<dt>Please follow these rules:</dt>
<dd><ul class="first last simple">
<li>One row per object.</li>
<li>Each row is class x_center y_center width height format.</li>
<li>Box coordinates must be in normalized xywh format (from 0 - 1).</li>
<li>Classes are zero-indexed (start from 0).</li>
</ul>
</dd>
</dl>
<ol class="arabic simple" start="2">
<li>Separate data in train and test set.</li>
<li>Replace <code class="docutils literal notranslate"><span class="pre">.names</span></code> file and <code class="docutils literal notranslate"><span class="pre">.data</span></code> file to fit your new classes.</li>
</ol>
<p>4. Each YOLO layer has 255 outputs: 85 outputs per anchor [4 box coordinates + 1 object confidence + 80 class confidences], times 3 anchors. If you use fewer classes, you can reduce this to <code class="docutils literal notranslate"><span class="pre">[4</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">+</span> <span class="pre">n]</span> <span class="pre">*</span> <span class="pre">3</span> <span class="pre">=</span> <span class="pre">15</span> <span class="pre">+</span> <span class="pre">3*n</span></code> outputs, where n is your class count.
This modification should be made to the output filter preceding each of the 3 YOLO layers. Also modify classes=80 to classes=n in each YOLO layer, where n is your class count.</p>
<ol class="arabic simple" start="5">
<li>Run darknet file normally.</li>
</ol>
</div>
<div class="section" id="fine-tuning-pre-trained-model-on-custom-dataset">
<h2>Fine-tuning pre-trained model on custom dataset<a class="headerlink" href="#fine-tuning-pre-trained-model-on-custom-dataset" title="Permalink to this headline">¶</a></h2>
<p>Note that fine-tuning is performed via top-down domain adaptation, where the weights are adapted to the new custom dataset distribution.</p>
<ol class="arabic simple">
<li>Prepare the dataset like described in section Training from scratch.</li>
<li>Set flag <code class="docutils literal notranslate"><span class="pre">random=1</span></code> in your .cfg file - it will increase precision by training Yolo for different resolutions.</li>
<li>Set learning rate to be drastically lower in the .cfg file <code class="docutils literal notranslate"><span class="pre">learning-rate=0.000001</span></code>.</li>
</ol>
<p>4. In the .cfg file, find the first appearance of <code class="docutils literal notranslate"><span class="pre">######################</span></code>.
Look at the shortcut layer above, and insert <code class="docutils literal notranslate"><span class="pre">stopbackward</span> <span class="pre">=</span> <span class="pre">1</span></code> (it is usually line 548).</p>
<ol class="arabic simple" start="5">
<li>Run darknet file normally.</li>
</ol>
</div>
<div class="section" id="my-experiment-with-fine-tuning-on-vedai-dataset">
<h2>My experiment with fine-tuning on VEDAI dataset<a class="headerlink" href="#my-experiment-with-fine-tuning-on-vedai-dataset" title="Permalink to this headline">¶</a></h2>
<dl class="docutils">
<dt>I have modified the VEDAI dataset and prepared it for the YOLOv3.</dt>
<dd><ul class="first last simple">
<li>Annotation files are available for <a class="reference external" href="https://drive.google.com/open?id=1EB_w6PUSEb_Sgh0FN9MoNVB9HUhh9Sti">Windows</a>, and for Linux on the official website.</li>
<li>You can find more information about the dataset itself <a class="reference external" href="https://downloads.greyc.fr/vedai/">here</a>.</li>
</ul>
</dd>
</dl>
<p>I have later performed fine-tuning like described, with using only the fraction of the data for the rest of the classes (Validation set from open images, can be downloaded <a class="reference external" href="https://storage.googleapis.com/openimages/web/index.html">here</a>).</p>
</div>
</div>
<div class="section" id="indices-and-search">
<h1>Indices and search<a class="headerlink" href="#indices-and-search" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Andjela Todorovic

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>